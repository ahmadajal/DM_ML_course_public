{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Analytics Tokenization - Stemming - Lemmatization\n",
        "\n",
        "There are two main packages for doing text processing and analytics with Python: NLTK and spaCy. SpaCy is the \"new kid on the block\" and should superior and faster results than NLTK, but for starting in text analytics both packages are good.\n",
        "\n",
        "We will play a bit with spaCy and NLTK."
      ],
      "metadata": {
        "id": "elyFltX-oVkt",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we update and install spaCy\n",
        "!pip install -U spacy"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "CQr7v6fNhoHA",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we load the english language model\n",
        "!python -m spacy download en"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "YZ_pEAARjZyF",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the spaCy language model."
      ],
      "metadata": {
        "id": "KG8LhYtyipIh",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "xNAhMeLliGX2",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "We create a simple spaCy document.\n"
      ],
      "metadata": {
        "id": "quYzhhajjJVu",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = sp(u'The Royal Swedish Academy of Sciences has awarded the Nobel Prize in Physics 2019 \"for contributions to our understanding of the evolution of the universe and Earth''s place in the cosmos\"')"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "9P63E582ij0B",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy automatically breaks your document into tokens when a document is created using the model.\n",
        "\n",
        "A token simply refers to an individual part of a sentence having some semantic value. Let's see what tokens we have in our document:"
      ],
      "metadata": {
        "id": "0N9yjY7hjzUQ",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in sentence:\n",
        "    print(word.text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "Royal\n",
            "Swedish\n",
            "Academy\n",
            "of\n",
            "Sciences\n",
            "has\n",
            "awarded\n",
            "the\n",
            "Nobel\n",
            "Prize\n",
            "in\n",
            "Physics\n",
            "2019\n",
            "\"\n",
            "for\n",
            "contributions\n",
            "to\n",
            "our\n",
            "understanding\n",
            "of\n",
            "the\n",
            "evolution\n",
            "of\n",
            "the\n",
            "universe\n",
            "and\n",
            "Earths\n",
            "place\n",
            "in\n",
            "the\n",
            "cosmos\n",
            "\"\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "E39zh5bwjRGp",
        "colab_type": "code",
        "outputId": "66e7e428-b327-4939-d105-d1c5fd5a9589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see we have the following tokens in our document. We can also see the parts-of-speech (POS) of each of these tokens using the `.pos_` attribute shown below. POS tagging can be really useful, particularly if you have words or tokens that can have multiple POS tags. For instance, the word \"fish\" can be used as both a noun and verb, depending upon the context."
      ],
      "metadata": {
        "id": "mGl8bsV6j7k8",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1= sp(\"I like to fish\")\n",
        "\n",
        "for word in s1:\n",
        "    print(word.text,  word.pos_)\n",
        "print()\n",
        "s2= sp(\"The fish jumped out of my hand\")\n",
        "for word in s2:\n",
        "    print(word.text,  word.pos_)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I PRON\n",
            "like VERB\n",
            "to PART\n",
            "fish VERB\n",
            "\n",
            "The DET\n",
            "fish NOUN\n",
            "jumped VERB\n",
            "out ADP\n",
            "of ADP\n",
            "my DET\n",
            "hand NOUN\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "iXBkABx6j2RW",
        "colab_type": "code",
        "outputId": "ad81b5c7-161f-4324-cf53-0c182babdf33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You see how powerful POS tagging is! \n",
        "\n",
        "Let's visualize it.\n",
        "\n"
      ],
      "metadata": {
        "id": "ioXnaXJ9kLk-",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "sen = sp(u\"The fish jumped out of my hand\")\n",
        "displacy.render(sen, style='dep', jupyter=True, options={'distance': 85})"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c4be1d5603fb4dd4b00a5b4741fe2a91-0\" class=\"displacy\" width=\"645\" height=\"222.0\" direction=\"ltr\" style=\"max-width: none; height: 222.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"135\">fish</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"135\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"220\">jumped</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"220\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"305\">out</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"305\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"390\">of</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"390\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"475\">my</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"475\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"132.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"560\">hand</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"560\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-0\" stroke-width=\"2px\" d=\"M70,87.0 C70,44.5 130.0,44.5 130.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,89.0 L62,77.0 78,77.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-1\" stroke-width=\"2px\" d=\"M155,87.0 C155,44.5 215.0,44.5 215.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M155,89.0 L147,77.0 163,77.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-2\" stroke-width=\"2px\" d=\"M240,87.0 C240,44.5 300.0,44.5 300.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M300.0,89.0 L308.0,77.0 292.0,77.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-3\" stroke-width=\"2px\" d=\"M325,87.0 C325,44.5 385.0,44.5 385.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M385.0,89.0 L393.0,77.0 377.0,77.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-4\" stroke-width=\"2px\" d=\"M495,87.0 C495,44.5 555.0,44.5 555.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M495,89.0 L487,77.0 503,77.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-5\" stroke-width=\"2px\" d=\"M410,87.0 C410,2.0 560.0,2.0 560.0,87.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-c4be1d5603fb4dd4b00a5b4741fe2a91-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M560.0,89.0 L568.0,77.0 552.0,77.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "IMLaowjn5M9q",
        "colab_type": "code",
        "outputId": "6e36f0e0-300e-4cd6-a09e-47093359201a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create another sentence\n",
        " sentence2 = sp(u\"She isn't looking to buy an apartment.\")\n",
        "\n",
        "# and we print out the dependences\n",
        " for word in sentence2:\n",
        "    print(f'{word.text:{12}} {word.pos_:{10}} {word.tag_:{8}} {spacy.explain(word.tag_)}')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "She          PRON       PRP      pronoun, personal\n",
            "is           VERB       VBZ      verb, 3rd person singular present\n",
            "n't          ADV        RB       adverb\n",
            "looking      VERB       VBG      verb, gerund or present participle\n",
            "to           PART       TO       infinitival to\n",
            "buy          VERB       VB       verb, base form\n",
            "an           DET        DT       determiner\n",
            "apartment    NOUN       NN       noun, singular or mass\n",
            ".            PUNCT      .        punctuation mark, sentence closer\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "HPBKmlWoj_UL",
        "colab_type": "code",
        "outputId": "5e76d0c4-1ef1-4ee4-8e8a-1671b1c683fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the isn't becomes 2 tokens.\n",
        "\n",
        "You can also break down a document in sentences."
      ],
      "metadata": {
        "id": "1qGn6ITclB_T",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = sp(u'The Big Bang model describes the universe from its very first moments.  Even today, this ancient radiation is all around us.')\n",
        "for i,sentence in enumerate(document.sents):\n",
        "    print(i, \":\", sentence); "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : The Big Bang model describes the universe from its very first moments.  \n",
            "1 : Even today, this ancient radiation is all around us.\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "MiNYQyCwkxkp",
        "colab_type": "code",
        "outputId": "841b225f-d832-45f7-af94-1157f91ca38d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# more elaborate tokenization\n",
        "sentence3 = sp(u'I\\'m leaving U.K. for U.S.A.')\n",
        "for word in sentence3:\n",
        "    print(word.text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n",
            "'m\n",
            "leaving\n",
            "U.K.\n",
            "for\n",
            "U.S.A.\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "wOC6aqzhmHP2",
        "colab_type": "code",
        "outputId": "6548bbcc-21ca-4f28-c807-567d2b0e7247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You see that U.K. and U.S.A. are correctly recognized as different token and not split into several ones."
      ],
      "metadata": {
        "id": "UItT9wCBmVIP",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# and another one\n",
        "\n",
        "sentence4 = sp(u\"Hello, I am Michalis from Zurich, Switzerland, email me at michalis@gmail.com\")\n",
        "for word in sentence4:\n",
        "    print(word.text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello\n",
            ",\n",
            "I\n",
            "am\n",
            "Michalis\n",
            "from\n",
            "Zurich\n",
            ",\n",
            "Switzerland\n",
            ",\n",
            "email\n",
            "me\n",
            "at\n",
            "michalis@gmail.com\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "axX9QozemfsR",
        "colab_type": "code",
        "outputId": "8393efe9-6e99-464c-d17b-51fb0af01ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The email was correctly recognized as one token."
      ],
      "metadata": {
        "id": "I5_r7mPSmuPb",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming refers to reducing a word to its root form. While performing natural language processing tasks, you will encounter various scenarios where you find different words with the same root. For instance, compute, computer, computing, computed, etc. You may want to reduce the words to their root form for the sake of uniformity. This is where stemming comes in to play.\n",
        "\n",
        "SpaCy does not include stemming; we will use NLTK. The most popular stemmer (for English) is the \"Porter Stemmer\". "
      ],
      "metadata": {
        "id": "MZMOrvKqoQEt",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "tokens = ['compute', 'computer', 'computed', 'computing']\n",
        "for token in tokens:\n",
        "    print(token + ' --> ' + stemmer.stem(token))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute --> comput\n",
            "computer --> comput\n",
            "computed --> comput\n",
            "computing --> comput\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "-TJuS5lkng47",
        "colab_type": "code",
        "outputId": "ff296474-223a-450e-d53b-99736f4dd285",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "\n",
        "Lemmatization is less aggressive than stemming."
      ],
      "metadata": {
        "id": "2lqFI9MJE6A1",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = sp(u'run runs running runner talks talk talking talked')\n",
        "\n",
        "for word in sentence:\n",
        "    print(word.text,\" --> \", word.lemma_)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run  -->  run\n",
            "runs  -->  run\n",
            "running  -->  run\n",
            "runner  -->  runner\n",
            "talks  -->  talk\n",
            "talk  -->  talk\n",
            "talking  -->  talk\n",
            "talked  -->  talk\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "sTj0nCsDrk-o",
        "colab_type": "code",
        "outputId": "da587f4a-14b8-4f02-ebfd-e861cc606441",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords\n",
        "\n",
        "Stop words are English words such as \"the\", \"a\", \"an\" etc that do not have any meaning of their own. Stop words are often not very useful for NLP tasks such as text classification or language modeling. So it is often better to remove these stop words before further processing of the document.\n",
        "\n",
        "The spaCy library contains 305 stop words. In addition, depending upon our requirements, we can also add or remove stop words from the spaCy library.\n",
        "\n",
        "To see the default spaCy stop words, we can use stop_words attribute of the spaCy model as shown below:"
      ],
      "metadata": {
        "id": "h2dVidUXJYo0",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "#Printing the total number of stop words:\n",
        "print('Number of stop words: %d' % len(spacy_stopwords))\n",
        "\n",
        "#Printing first ten stop words:\n",
        "print('First ten stop words: %s' % list(spacy_stopwords)[:20])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of stop words: 326\n",
            "First ten stop words: ['us', 'whence', 'thereupon', 'you', 'hereafter', 'then', 'a', 'never', 'noone', 'up', 'however', 'in', 'put', 'will', '‘ll', 'himself', 'i', 'thence', 'bottom', \"'re\"]\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "uU3wJ72s84oU",
        "colab_type": "code",
        "outputId": "016cc963-4850-418c-e224-4fae05a1f096",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# typically we should remove stopwords from our text.\n",
        "\n",
        "text = \"There are many documents that contain stopwords they are not very useful\"\n",
        "filtered_sentence=[]\n",
        "doc = sp(text)\n",
        "\n",
        "# filtering stop words\n",
        "for word in doc:\n",
        "    if word.is_stop==False:\n",
        "        filtered_sentence.append(word)\n",
        "print(\"Filtered Sentence:\",filtered_sentence)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Sentence: [documents, contain, stopwords, useful]\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "G6clFzRI9vZP",
        "colab_type": "code",
        "outputId": "69e57b06-2e87-4c99-d7d0-d30fccc360b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# is a word a stopword?\n",
        "sp.vocab['wonder'].is_stop"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "kgC6FqFP-V7j",
        "colab_type": "code",
        "outputId": "74818fe3-6608-45cd-bc5f-f7210b83c0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecting entities\n",
        "\n",
        "While we are at it, we can see that it's very easy to detect entities (this is called **Named Entity Recognition**. SpaCy, comes with a pre-trained classifier that detects important entities: location, time, people, money etc.\n",
        "\n",
        "To get the named entities from a document, you have to use the `ents` attribute. Let's retrieve the named entities from the above sentence. Execute the following script:"
      ],
      "metadata": {
        "id": "eiHLtf4fl8PU",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for entity in sentence4.ents:\n",
        "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Michalis - PERSON - People, including fictional\n",
            "Zurich - GPE - Countries, cities, states\n",
            "Switzerland - GPE - Countries, cities, states\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "1Z3CT7eAlnbY",
        "colab_type": "code",
        "outputId": "46f5cd45-3785-4cdd-c932-d2892388e404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "Use the sentence below. \n",
        "\n",
        "- How many tokens does it have?\n",
        "- How many entities are recognized?\n"
      ],
      "metadata": {
        "id": "XqRr67YpqK2o",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'This year''s Nobel Prize in economics was awarded to three scholars who revolutionized the effort to end global poverty: Abhijit Banerjee and Esther Duflo of MIT and Michael Kremer of Harvard are essentially credited with applying the scientific method to an enterprise that, until recently, was largely based on gut instincts.'\n",
        "\n",
        "# show them. how many tokens\n",
        "\n",
        "# show them. how many entities\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This years - DATE - Absolute or relative dates or periods\n",
            "Nobel Prize - WORK_OF_ART - Titles of books, songs, etc.\n",
            "three - CARDINAL - Numerals that do not fall under another type\n",
            "Abhijit Banerjee - PERSON - People, including fictional\n",
            "Esther Duflo - PERSON - People, including fictional\n",
            "MIT - ORG - Companies, agencies, institutions, etc.\n",
            "Michael Kremer - PERSON - People, including fictional\n",
            "Harvard - ORG - Companies, agencies, institutions, etc.\n"
          ]
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "baDKZ2EVqPfp",
        "colab_type": "code",
        "outputId": "bbaa6e2e-4fc0-4482-835e-b64c9721aae9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Entities and showing them in the text\n",
        "\n",
        "Of course there are many more entities that we can detect in a text. Let's see an example."
      ],
      "metadata": {
        "id": "jFL5cELt6OAk",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "nytimes= sp(u\"\"\"New York City on Tuesday declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.\n",
        "\n",
        "At least 285 people have contracted measles in the city since September, mostly in Brooklyn’s Williamsburg neighborhood. The order covers four Zip codes there, Mayor Bill de Blasio (D) said Tuesday.\n",
        "\n",
        "The mandate orders all unvaccinated people in the area, including a concentration of Orthodox Jews, to receive inoculations, including for children as young as 6 months old. Anyone who resists could be fined up to $1,000.\"\"\")\n",
        "\n",
        "entities=[(i, i.label_, i.label) for i in nytimes.ents]\n",
        "entities"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": [
              "[(New York City, 'GPE', 384),\n",
              " (Tuesday, 'DATE', 391),\n",
              " (At least 285, 'CARDINAL', 397),\n",
              " (September, 'DATE', 391),\n",
              " (Brooklyn, 'GPE', 384),\n",
              " (Williamsburg, 'GPE', 384),\n",
              " (four, 'CARDINAL', 397),\n",
              " (Bill de Blasio, 'PERSON', 380),\n",
              " (Tuesday, 'DATE', 391),\n",
              " (Orthodox Jews, 'NORP', 381),\n",
              " (6 months old, 'DATE', 391),\n",
              " (up to $1,000, 'MONEY', 394)]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "qcEnXXOY6RRv",
        "colab_type": "code",
        "outputId": "48e346b4-9a5f-43a6-a9f6-4e955049e67d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(nytimes, style = \"ent\",jupyter = True)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    New York City\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " on \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Tuesday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " declared a public health emergency and ordered mandatory measles vaccinations amid an outbreak, becoming the latest national flash point over refusals to inoculate against dangerous diseases.</br></br>\n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    At least 285\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " people have contracted measles in the city since \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    September\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", mostly in \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Brooklyn\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "’s \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Williamsburg\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " neighborhood. The order covers \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    four\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " Zip codes there, Mayor \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Bill de Blasio\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " (D) said \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Tuesday\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".</br></br>The mandate orders all unvaccinated people in the area, including a concentration of \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    Orthodox Jews\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              ", to receive inoculations, including for children as young as \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    6 months old\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ". Anyone who resists could be fined \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
              "    up to $1,000\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              ".</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "kIIM1Ro16heq",
        "colab_type": "code",
        "outputId": "0364fdab-191f-48d4-80ac-5d4f2fdb856e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see that all the 4 words have been reduced to \"comput\" which actually isn't a word (but it can be considered as a token), and it does show that all 4 words have something in common. "
      ],
      "metadata": {
        "id": "4i5OlqStruh-",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Representation (Bag of words)"
      ],
      "metadata": {
        "id": "JIPLS2BD-gHM",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "texts = [ \"I like the Matrix and the Patriot\", \"I did not like the Ants movie\", \"I hate comedies, but the \"]\n",
        "# texts = [\n",
        "#     \"Walks like a duck, talks like a duck\", \n",
        "#     \"Beijing duck is the dish I like\",\n",
        "#     \"Roger Rabbit has the recipe of success\",\n",
        "#     \"A recipe for rabbit\",\n",
        "#     \"A recipe for Beijing duck\"\n",
        "# ]\n",
        "\n",
        "# using default tokenizer \n",
        "count = CountVectorizer(ngram_range=(1,2))\n",
        "bow = count.fit_transform(texts)\n",
        "\n",
        "# Show feature matrix\n",
        "bow.toarray()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": [
              "array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 2, 0,\n",
              "        1, 1],\n",
              "       [0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
              "        0, 0],\n",
              "       [0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "        0, 0]], dtype=int64)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "id": "kXu3Cpcl-iyL",
        "colab_type": "code",
        "outputId": "0027fe35-c834-4c29-f2a4-936367a17ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature names\n",
        "feature_names = count.get_feature_names()\n",
        "\n",
        "# View feature names\n",
        "feature_names"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": [
              "['and',\n",
              " 'and the',\n",
              " 'ants',\n",
              " 'ants movie',\n",
              " 'but',\n",
              " 'but the',\n",
              " 'comedies',\n",
              " 'comedies but',\n",
              " 'did',\n",
              " 'did not',\n",
              " 'hate',\n",
              " 'hate comedies',\n",
              " 'like',\n",
              " 'like the',\n",
              " 'matrix',\n",
              " 'matrix and',\n",
              " 'movie',\n",
              " 'not',\n",
              " 'not like',\n",
              " 'patriot',\n",
              " 'the',\n",
              " 'the ants',\n",
              " 'the matrix',\n",
              " 'the patriot']"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "id": "Hdyz6l9Bmcum",
        "colab_type": "code",
        "outputId": "865864ac-3a7c-45f1-9ba8-fc2625b507c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show as a dataframe\n",
        "pd.DataFrame(\n",
        "    bow.todense(), \n",
        "    columns=feature_names\n",
        "    )"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": [
              "   and  and the  ants  ants movie  but  but the  comedies  comedies but  did  \\\n",
              "0    1        1     0           0    0        0         0             0    0   \n",
              "1    0        0     1           1    0        0         0             0    1   \n",
              "2    0        0     0           0    1        1         1             1    0   \n",
              "\n",
              "   did not  ...  matrix  matrix and  movie  not  not like  patriot  the  \\\n",
              "0        0  ...       1           1      0    0         0        1    2   \n",
              "1        1  ...       0           0      1    1         1        0    1   \n",
              "2        0  ...       0           0      0    0         0        0    1   \n",
              "\n",
              "   the ants  the matrix  the patriot  \n",
              "0         0           1            1  \n",
              "1         1           0            0  \n",
              "2         0           0            0  \n",
              "\n",
              "[3 rows x 24 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>and</th>\n",
              "      <th>and the</th>\n",
              "      <th>ants</th>\n",
              "      <th>ants movie</th>\n",
              "      <th>but</th>\n",
              "      <th>but the</th>\n",
              "      <th>comedies</th>\n",
              "      <th>comedies but</th>\n",
              "      <th>did</th>\n",
              "      <th>did not</th>\n",
              "      <th>...</th>\n",
              "      <th>matrix</th>\n",
              "      <th>matrix and</th>\n",
              "      <th>movie</th>\n",
              "      <th>not</th>\n",
              "      <th>not like</th>\n",
              "      <th>patriot</th>\n",
              "      <th>the</th>\n",
              "      <th>the ants</th>\n",
              "      <th>the matrix</th>\n",
              "      <th>the patriot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 24 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "id": "HKceAKN9mhn5",
        "colab_type": "code",
        "outputId": "93b81eeb-c08d-4659-89ec-53861318d0f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise:\n",
        "\n",
        "Above, we only used 1-grams (each word on each own). Change the code above to create a bag-of-words representation that includes \n",
        "\n",
        "    A. 1-grams and 2-grams.\n",
        "    B. 1,2,3-grams\n",
        "\n",
        "**Hint:** Use the `ngram_range` parameter in the `CountVectorizer`.\n",
        "\n",
        "What do you notice? How many features does your document-term matrix have now?\n"
      ],
      "metadata": {
        "id": "GMM4B-Njo6_l",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Representation\n",
        "\n",
        "Recall that:\n",
        "\n",
        "- term frequency tf = count(word, document) / len(document) \n",
        "- term frequency idf = log( len(collection) / count(document_containing_term, collection) )\n",
        "- tf-idf = tf * idf \n",
        "\n",
        "It is important to mention that the IDF value for a word remains the same throughout all the documents as it depends upon the total number of documents. On the other hand, TF values of a word differ from document to document.\n",
        "\n",
        "The TF for the word \"car\" is 1/7.\n",
        "\n",
        "Let's find the IDF frequency of the word \"car\". Since we have 2 documents and the word \"car\" occurs in 1 of them, therefore the IDF value of the word \"car\" is log(2/1) = 1.66.\n",
        "\n",
        "\n",
        "\n",
        "Finally, the TF-IDF values are calculated by multiplying TF values with their corresponding IDF values.\n",
        "\n",
        "**Note**: In the example below, you may not get the exact values by multiplying those two numbers, because nltk normalizes each row to have norm of 1. However the relative importance of the terms won't change.\n",
        "\n"
      ],
      "metadata": {
        "id": "8P98scElj88b",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "texts = [\n",
        "    \"The car is driven on the road.\", \n",
        "    \"The truck is driven on the highway\"\n",
        "]\n",
        "# using default tokenizer in TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names()\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": [
              "        car   driven   highway       is       on      road      the     truck\n",
              "0  0.424717  0.30219  0.000000  0.30219  0.30219  0.424717  0.60438  0.000000\n",
              "1  0.000000  0.30219  0.424717  0.30219  0.30219  0.000000  0.60438  0.424717"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>car</th>\n",
              "      <th>driven</th>\n",
              "      <th>highway</th>\n",
              "      <th>is</th>\n",
              "      <th>on</th>\n",
              "      <th>road</th>\n",
              "      <th>the</th>\n",
              "      <th>truck</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.424717</td>\n",
              "      <td>0.30219</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.30219</td>\n",
              "      <td>0.30219</td>\n",
              "      <td>0.424717</td>\n",
              "      <td>0.60438</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.30219</td>\n",
              "      <td>0.424717</td>\n",
              "      <td>0.30219</td>\n",
              "      <td>0.30219</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.60438</td>\n",
              "      <td>0.424717</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "id": "hP06zKDdj8aB",
        "colab_type": "code",
        "outputId": "7f54df83-1b92-4bac-de72-653bccf52605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combining NLTK and spaCy\n",
        "\n",
        "Of course you can combine the two tools."
      ],
      "metadata": {
        "id": "JKBL30NxyA2l",
        "colab_type": "text"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from html import unescape\n",
        "\n",
        "# create a dataframe from a word matrix\n",
        "def wordmatrix_2_df(wm, feat_names):\n",
        "    # create an index for each row\n",
        "    doc_names = ['Doc{:d}'.format(idx) for idx, _ in enumerate(wm)]\n",
        "    df = pd.DataFrame(data=wm.toarray(), index=doc_names,\n",
        "                      columns=feat_names)\n",
        "    return(df)\n",
        "\n",
        "# create a spaCy tokenizer\n",
        "spacy.load('en')\n",
        "lemmatizer = spacy.lang.en.English()\n",
        "\n",
        "# remove html entities from docs and\n",
        "# set everything to lowercase\n",
        "def my_preprocessor(doc):\n",
        "    return(unescape(doc).lower())\n",
        "\n",
        "# tokenize the doc and lemmatize its tokens\n",
        "def my_tokenizer(doc):\n",
        "    tokens = lemmatizer(doc)\n",
        "    return([token.lemma_ for token in tokens])\n",
        "\n",
        "corpora = ['University of Lausanne', 'University of Geneva', 'University of Zurich']\n",
        "\n",
        "custom_vec = CountVectorizer(preprocessor=my_preprocessor, tokenizer=my_tokenizer)\n",
        "cwm = custom_vec.fit_transform(corpora)\n",
        "tokens = custom_vec.get_feature_names()\n",
        "wordmatrix_2_df(cwm, tokens)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 41,
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>geneva</th>\n",
              "      <th>lausanne</th>\n",
              "      <th>of</th>\n",
              "      <th>university</th>\n",
              "      <th>zurich</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Doc0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Doc2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      geneva  lausanne  of  university  zurich\n",
              "Doc0       0         1   1           1       0\n",
              "Doc1       1         0   1           1       0\n",
              "Doc2       0         0   1           1       1"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": 0,
      "metadata": {
        "id": "1mgXgAcPkAwy",
        "colab_type": "code",
        "outputId": "6662a6de-9008-4b53-9801-8e5510facb88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "mEmheRmJyueh",
        "colab_type": "code",
        "colab": {}
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Text Analytics Tokenization - Stemming - Lemmatization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.15.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}