{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying hand-written digits with deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras package\n",
    "Keras is simply a wrapper around more complex numerical computation engines such as TensorFlow and Theano.\n",
    "\n",
    "Keras abstracts away much of the complexity of building a deep neural network, leaving us with a very simple, nice, and easy to use interface to rapidly build, test, and deploy deep learning architectures.\n",
    "\n",
    "When it comes to Keras you have two choices for a backend engine â€” either TensorFlow or Theano. Theano is older than TensorFlow and was originally the only choice when selecting a backend for Keras.\n",
    "TensorFlow is extremely flexible, allowing you to deploy network computation to multiple CPUs, GPUs, servers, or even mobile systems without having to change a single line of code. Therefore, it is preferable to install Keras on TensorFlow backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Keras on the TensorFlow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (5,5) # Make the figures a bit bigger\n",
    "from sklearn.datasets import load_digits\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1125)\n",
    "nb_classes = 10\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "plt.imshow(X[7].reshape((8,8)), plt.get_cmap('Greys_r'), interpolation='nearest')\n",
    "plt.show()\n",
    "plt.imshow(X[1].reshape((8,8)), plt.get_cmap('Greys_r'), interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2)\n",
    "#convert target labels to one-hot vectors, this is required if we want to use categorical cross-entropy\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes) \n",
    "Y_test = np_utils.to_categorical(Y_test, nb_classes)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a simple neural-network with Keras\n",
    "\n",
    "Source: https://github.com/wxs/keras-mnist-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras import optimizers\n",
    "np.random.seed(1125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the neural network\n",
    "Build the neural-network. Here we'll do a simple 3 layer fully connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(64,)))\n",
    "model.add(Activation('relu')) # An \"activation\" is just a non-linear function applied to the output\n",
    "                              # of the layer above. Here, with a \"rectified linear unit\",\n",
    "                              # we clamp all values below 0 to 0.\n",
    "                           \n",
    "model.add(Dropout(0.2))   # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax')) # This special \"softmax\" activation among other things,\n",
    "                                 # ensures the output is a valid probaility distribution, that is\n",
    "                                 # that its values are all non-negative and sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "Keras is built on top of TensorFlow. This package allows you to define a *computation graph* in Python, which can be compiled and run efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
    "\n",
    "When compiing a model, Keras asks you to specify your **loss function** and your **optimizer**. The loss function we'll use here is called *categorical crossentropy*, and is a loss function well-suited to comparing two probability distributions.\n",
    "\n",
    "Here our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. [More detail at Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)\n",
    "\n",
    "The optimizer helps determine how quickly the model learns, how resistent it is to getting \"stuck\" or \"blowing up\". Keras provides common optimizer functions as documented [here](https://keras.io/optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model!\n",
    "This is the fun part: you can feed the training data loaded in earlier into this model and it will learn to classify digits. We use 20% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_hist = model.fit(X_train, Y_train,\n",
    "                       batch_size=64, epochs=20,\n",
    "                       verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(model_hist.history['val_accuracy']) # or model_hist.history['val_acc'] depending on tf version\n",
    "plt.xlabel(\"#Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"validation accuracy in different epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of the model on the unseen test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the output\n",
    "\n",
    "It's always a good idea to inspect the output and make sure everything looks sane. Here we'll look at some examples it gets right, and some examples it gets wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict_classes function outputs the highest probability class\n",
    "# according to the trained classifier for each input example.\n",
    "predicted_classes = model.predict_classes(X_test)\n",
    "\n",
    "# Check which items we got right / wrong\n",
    "actual_classes = np.where(Y_test==1)[1]\n",
    "\n",
    "correct_indices = np.nonzero(predicted_classes == actual_classes)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != actual_classes)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(5, 5))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(X_test[correct_indices[i]].reshape(8,8), cmap='gray', interpolation='none')\n",
    "    ax.set_title(\"Pred: {}, True: {}\".format(predicted_classes[correct_indices[i]], actual_classes[correct_indices[i]]))\n",
    "plt.suptitle('Correct Predictions', y=1.05, size=14)\n",
    "plt.tight_layout()\n",
    "    \n",
    "fig, axes = plt.subplots(2, 3, figsize=(5, 5))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.imshow(X_test[incorrect_indices[i]].reshape(8,8), cmap='gray', interpolation='none')\n",
    "    ax.set_title(\"Pred: {}, True: {}\".format(predicted_classes[incorrect_indices[i]], actual_classes[incorrect_indices[i]]))\n",
    "plt.suptitle('Incorrect Predictions', y=1.05, size=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of other great examples at the Keras homepage at http://keras.io and in the source code at https://github.com/fchollet/keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
