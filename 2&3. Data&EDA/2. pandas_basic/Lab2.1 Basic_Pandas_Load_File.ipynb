{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Pandas operations\n",
    "\n",
    "**Goal**: Our goal here is to learn how to load a dataset into a Pandas DataFrame. The dataset can come either in CSV or in JSON format. We will see also how to perform basic data manipulations and very basic data visualizations so that you understand the nature of your data.\n",
    "\n",
    "## 1. Loading a dataset in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('../data/pandas_tutorial_read.csv') \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the above correct? Most likely not. We see there are ';' and the data seem to be in one column. The default delimiter is ',' so we need to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/pandas_tutorial_read.csv', delimiter=';') \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better. But something else does not look good now. Now we don't have a header for the data. This usually comes with the documentation. I provide it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/pandas_tutorial_read.csv', \n",
    "                   delimiter=';', \n",
    "                   names = ['my_datetime', 'event', 'country', 'user_id', 'source', 'topic']) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have to have the data locally you can also load a csv file from a url. In this case, it is not stored locally though. This is especially useful if you are using colab, in this case you can copy the url of the raw data in github and load the data in pandas directly from github.\n",
    "\n",
    "```python\n",
    "url = \"some url\"\n",
    "data = pd.read_csv(url, \n",
    "                   delimiter=';', \n",
    "                   names = ['my_datetime', 'event', 'country', 'user_id', 'source', 'topic']) \n",
    "data.head()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/ahmadajal/DM_ML_course_public/master/2.%20Data%26EDA/data/pandas_tutorial_read.csv\"\n",
    "data = pd.read_csv(url, \n",
    "                   delimiter=';', \n",
    "                   names = ['my_datetime', 'event', 'country', 'user_id', 'source', 'topic']) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see \n",
    "\n",
    "- whole dataset: ```data```\n",
    "- the beginning ```head()``` \n",
    "- the ```tail()``` or \n",
    "- a ```sample(5)```\n",
    "\n",
    "Try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame components\n",
    "There are three components of the DataFrame: the index, columns and data (values). We can extract each of these components into their own variables. Let's do that and then inspect them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = data.index\n",
    "columns = data.columns\n",
    "values = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types of the components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The index and the columns are the same type: a pandas **`Index`** object (**`RangeIndex`** is of type **`Index`**), which is a sequence of labels for either the rows or the columns.\n",
    "\n",
    "The values are a NumPy **`ndarray`**, which stands for n-dimensional array, and is the primary container of data in the NumPy library. Pandas is built directly on top of NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting columns\n",
    "\n",
    "If you want to select two particular columns you can do it like that:\n",
    "\n",
    "```data[['country', 'user_id']]``` \n",
    "\n",
    "or you can take the columns in a different order: \n",
    "\n",
    "```data[['user_id', 'country']]```.\n",
    "\n",
    "The way to remember the syntax is that outer brackets signify that you want to select columns, and the inner brackets are for the list itself.\n",
    "\n",
    "Try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above returns a pandas.DataFrame. If you want to return a pandas.Series instead then you can use this syntax:\n",
    "\n",
    "```data.user_id ```\n",
    "\n",
    "or \n",
    "\n",
    "``` data['user_id'] ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to filter one those users that came from SEO then you can write:\n",
    "\n",
    "``` data[data.source == 'SEO'] ```\n",
    "\n",
    "where the inner statement will create a boolean mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chaining\n",
    "\n",
    "You can combine multiple selection methods as follows:\n",
    "\n",
    "``` data.head()[['country', 'user_id']] ```\n",
    "\n",
    "**CAUTION**: A thing to keep in mind is that when you use chaining you work on *copies* of the original DataFrame. So if you use chaining to change data, you may observe that the original DataFrame was not changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now it's your turn to solve an exercise and deepen your knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Exercise 1:</h2>\n",
    "\n",
    "\n",
    "    \n",
    ">Select the user_id, the country and the topic columns for the users who are from country_2, and show only the first 10 rows\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your solution here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# possible solution \n",
    "# data[['user_id', 'topic', 'country']][data['country'] == 'country_2'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Loading JSON files\n",
    "\n",
    "Many of the data in the Internet exists in JSON format which is a structured text format, and is very similar to a Python dictionary.\n",
    "\n",
    "We will see how to load a JSON dataset in a Pandas DataFrame.\n",
    "\n",
    "We will use the Citibike API that provides a real-time view of the Citibike stations in New York.\n",
    "The API call at http://www.citibikenyc.com/stations/json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'http://www.citibikenyc.com/stations/json'\n",
    "data = requests.get(url).json()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you see how the JSON file looks. The JSON results contain two keys: The `executionTime` and `stationBeanList`. The `stationBeanList` is a list of dictionaries, with each dictionary corresponding to a Citibike station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Pandas we can easily convert a list of dictionaries into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.DataFrame(data[\"stationBeanList\"])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to understand the columns:\n",
    "\n",
    "We notice that:\n",
    "\n",
    "- **totalDocks** = **availableBikes** (bikes ready to rent) + **availableDocks** (how many docks are free)\n",
    "\n",
    "To see if the data has been imported correctly, we can verify the datatypes of the columns. Pandas tries to infer the datatypes and for this case it does a pretty good job. In general, you should consider providing explicitly the datatypes of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One column that looks not parsed correctly is the **lastCommunicationTime** which is an `object` (i.e., `string`), so you may want to convert it to the `datetime` type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Exercise 2:</h2>\n",
    "\n",
    "\n",
    "    \n",
    ">Convert the **lastCommunicationTime** into a `datetime` datatype. <br>\n",
    "**Hint**: Use the [pandas.to_datetime](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) function.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"lastCommunicationTime\"] = pd.to_datetime(df[\"lastCommunicationTime\"])\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that the **lastCommunicationTime** column is of type `datetime`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a column\n",
    "\n",
    "We can add a column `perc_full` that shows how full is each station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"perc_full\"] = df['availableBikes']/df['totalDocks']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Now we can do some basic visualizations to get a feeling of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some basic styling and importing of the right styles\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 10)\n",
    "matplotlib.style.use(['seaborn-talk', 'seaborn-ticks', 'seaborn-whitegrid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "Let's start with a histogram of the sizes of the bike docks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"totalDocks\"].hist(alpha=0.75);\n",
    "# alpha indicates the transparency of the bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, most of the bike stations have places for around 30 bikes.\n",
    "Now, let's do a histogram of the available bikes across bike stations (real-time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"availableBikes\"].hist(alpha=0.75);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot a histogram of how full each station is using the column we previously created `df[\"perc_full\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"perc_full\"].hist(alpha=0.5, bins=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot\n",
    "\n",
    "A scatter plot is useful for visualizing two variables at the same time. They are usually used to understand *correlations* between variables, but now we will use it as an elementary plotting function, instead of plotting directly on the map which is more complicated. \n",
    "\n",
    "We will use the longitude and latitude coordinates to plot the location of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(\n",
    "    kind='scatter', \n",
    "    x='longitude', \n",
    "    y='latitude',\n",
    "    figsize = (10,10)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h2>Exercise 3:</h2>\n",
    "\n",
    "<br>    \n",
    "Adjust the above graph to convey more information. Now the size of the marker in the scatterplot will communicate how full each station is. <br>\n",
    "**HINT:** The *s* parameter in the plot command controls the size of the market. We will make the size proportional to the perc_full column\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The s parameter controls the size of the market. We make the size proportional to the perc_full column\n",
    "df.plot(\n",
    "    kind='scatter', \n",
    "    x='longitude', \n",
    "    y='latitude', \n",
    "    s = 100*df['perc_full'], alpha=0.75,\n",
    "    figsize = (10,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "You can also use the `describe` function of Pandas to get some general understanding of the central values and the tendencies of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should ask yourself. **Are the values in the summary statistics what you expected them to be?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Writing the data to a CSV\n",
    "\n",
    "With the above, we just scratched the surface of what it means to do data processing.\n",
    "\n",
    "After you did your basic data processing, you may want to save the DataFrame in a new CSV file, so that you don't have to repeat the same pre-processing everytime. You can use the [to_csv](https://datatofish.com/export-dataframe-to-csv/) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following to save the file\n",
    "# df.to_csv(\"my_new_file.csv\", sep=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nteract": {
   "version": "0.14.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
