{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN classifier\n",
    "\n",
    "The k-nearest neighbors algorithm (k-NN) is a non-parametric method used for classification (and also regression). In k-NN classification, The input consists of the k closest training examples in the feature space. \n",
    "The output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "We will use the iris data-set which we can directly load from sklearn library. This data-set has 3 class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First KNN classifier\n",
    "let's train a KNN classifier with $k = 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know there is no training involved in knn classification. It is just the data features and class labels that we memorize and use them to classifiy a new unseen data point (or a query). Therefore the concept of train/test splitting is quiet non-sense here. However we still split the data to use the data points in the test set as queries to the knn classifer to compute its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test accuracy\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary\n",
    "\n",
    "As we want to show the decision boundary in a 2D plot, we only use the first 2 dimensions of the iris data-set and fit the knn classifier on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_X = iris.data[:, :2]\n",
    "disp_y = iris.target\n",
    "knn.fit(disp_X, disp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "# defining color-maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = disp_X[:, 0].min() - 1, disp_X[:, 0].max() + 1\n",
    "y_min, y_max = disp_X[:, 1].min() - 1, disp_X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure()\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=disp_y, cmap=cmap_bold,\n",
    "            edgecolor='k', s=20)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"3-Class classification (k = %i)\"\n",
    "          % (5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the value of $k$: over-fitting / under-fitting\n",
    "The choice of $k$ in KNN classifier can lead to over-fitting or under-fitting. Therefore we need to tune the value of $k$.\n",
    "\n",
    "#### Exercise:\n",
    "Tune the value of $k$ and find the best value. What is the precision and recall for the KNN classifier with the best $k$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in range(1, 100, 5):\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(range(1, 100, 5), scores)\n",
    "plt.ylabel('accuracy', fontsize=15)\n",
    "plt.xlabel('$k$', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would happen if we set $k = $ size of training set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundaries for different value of $k$\n",
    "\n",
    "Let's visualize the decision boundaries for $k = 1, 20, 60, 90$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(10,7));\n",
    "ax = ax.reshape(4,-1)\n",
    "for i, k in enumerate([1, 20, 60, 90]):\n",
    "    clf = KNeighborsClassifier(n_neighbors=k)\n",
    "    clf.fit(disp_X, disp_y)\n",
    "    x_min, x_max = disp_X[:, 0].min() - 1, disp_X[:, 0].max() + 1\n",
    "    y_min, y_max = disp_X[:, 1].min() - 1, disp_X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax[i,0].pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax[i,0].scatter(X[:, 0], X[:, 1], c=disp_y, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    ax[i,0].set_xlim(xx.min(), xx.max())\n",
    "    ax[i,0].set_ylim(yy.min(), yy.max())\n",
    "    ax[i,0].set_title(\"$k$ = {}\".format(k))\n",
    "ax = ax.reshape(2,2)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: skewed classes\n",
    "\n",
    "Skewed classes happens when we have unbalanced number of classes in our data (eg: we have more training example of one class compared to the others). In such situations KNN classifiers performance in predicting queris degrade. This is due to the majority voting that KNN classifier uses to predict the class label of a query.\n",
    "\n",
    "Below we have splitted the data to train and test set manually so that class labels 0 and 1 are over-represented in the training set. Apply a KNN classifier with $k = 16$ and observe the accuracy on the test set (queries). How would you resolve this problem?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0_train_indices = np.random.permutation(np.where(y==0)[0])[:45]\n",
    "y0_test_indices = np.random.permutation(np.where(y==0)[0])[45:]\n",
    "#-----\n",
    "y1_train_indices = np.random.permutation(np.where(y==1)[0])[:45]\n",
    "y1_test_indices = np.random.permutation(np.where(y==1)[0])[45:]\n",
    "#-----\n",
    "y2_train_indices = np.random.permutation(np.where(y==2)[0])[:30]\n",
    "y2_test_indices = np.random.permutation(np.where(y==2)[0])[30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = np.concatenate((y0_test_indices, y1_test_indices, y2_test_indices))\n",
    "train_indices = np.concatenate((y0_train_indices, y1_train_indices, y2_train_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_skewed = X[train_indices]\n",
    "y_train_skewed = y[train_indices]\n",
    "X_test_skewed = X[test_indices]\n",
    "y_test_skewed = y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=16, weights='uniform')\n",
    "clf.fit(X_train_skewed, y_train_skewed)\n",
    "print(\"accuracy = \", clf.score(X_test_skewed, y_test_skewed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to improve the accuracy for this skewed data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: improve the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Predicting churn with KNN classifier\n",
    "\n",
    "Now let's try to observe how the knn clssifier performs on predicting churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "First we load the data and remove the rows which don't have any value in the `TotalCharges` column (As we did in Assignment 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"PATH_TO_CHURN_DATA\") #or load it from github\n",
    "z = df[\"TotalCharges\"].map(lambda x: x.replace('.', '', 1).isdigit()) #check if the string contains digits (the dtype of this is object )\n",
    "df = df[z]\n",
    "df.reset_index(inplace=True)\n",
    "print(df.shape)\n",
    "df[\"TotalCharges\"] = df[\"TotalCharges\"].astype(float)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Encoding\n",
    "\n",
    "Apply the categorical encoding of your choice to the categorical features. Note that we will use the same features as of Assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: categorical encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test split\n",
    "\n",
    "Split your data to train and test sets. Take 20% of the data for the test set. It is better to fix the `random_state` to ensure reproducibility of your results! (However, note that in general your learning algorithm should perform (almost) in the same way with any random split of the data give that the split soes not make the data imbalanced and there are sufficiently large number of data samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning\n",
    "Find the best `k` for a KNN classifier trained on this data-set. Also play with other parameters of the KNN classifier, eg: what is the difference between giving a *uniform* weights to the points in a neighbourhood and giving weights based on the inverse distance (between a point in the neigbourhood and the query point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundary\n",
    "Visualize the decision boundary of your best KNN classifier. You can use the same code as above to plot the decision boundary. Note that for ploting the decision boundary you need to train the classifier with only 2 features. which features would you choose?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: decision boundary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
