{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User-User Collaborative Filtering From Scratch\n",
    "Here you will implement User-User Collaborative Filtering from scratch i.e. by only using numpy and scipy libraries. We will use the MovieLens 20M dataset.\n",
    "\n",
    "**PREREQUISITE:** Download the MovieLens 20M dataset from <http://files.grouplens.org/datasets/movielens/ml-20m.zip>. Extract the contents of the zip file as a folder named `data` located in the same folder as this notebook. You should have a file `ml-20m/ratings.csv`, which is what we'll be working with. \n",
    "\n",
    "**TASK:** Your job is to understand the notebook and also *fill in the missing code*. The place to enter your code is clearly marked with comments (try to do these parts, you will learn a lot!).\n",
    "\n",
    "**Note:** Wehenevr we set the `DEBUG` variable to True, it means that we are going to use a subsample of the ratings. This is only in order to reduce the execution time of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the data\n",
    "!pip install wget\n",
    "!mkdir data\n",
    "import wget\n",
    "filename = wget.download(\"http://files.grouplens.org/datasets/movielens/ml-20m.zip\", out=\"data/ml-20m.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the zip file to get the data-sets\n",
    "!unzip data/ml-20m.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules\n",
    "Import python modules we will be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse as sp\n",
    "from scipy.sparse.linalg import norm\n",
    "import sklearn.preprocessing as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set some formatting options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=500, precision=4)\n",
    "pd.options.display.max_seq_items = 20\n",
    "pd.options.display.max_rows = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "Load data from csv file located in `ml-100k/u.data`. The result is stored in a `pandas.DataFrame` called `ratings_raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_raw = pd.read_csv('ml-20m/ratings.csv')\n",
    "movies = pd.read_csv('ml-20m/movies.csv')\n",
    "links = pd.read_csv('ml-20m/links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ratings_raw.head())\n",
    "display(movies.head())\n",
    "display(links.head())\n",
    "print(\"Distinct users:\", len(ratings_raw.userId.unique()))\n",
    "print(\"Distinct items:\", len(ratings_raw.movieId.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets visualize some of the movie data. We will use [tmdbsimple](https://pypi.org/project/tmdbsimple/) which is a wrapper, written in Python, for The Movie Database (TMDb) API v3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may already have this package as we used it in week 2\n",
    "!pip install tmdbsimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmdb_class import TMDB # we have provided a python class called tmdb_class.py to retrieve the url of poster images\n",
    "from IPython.display import Image\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# which movies did user watch?\n",
    "def make_html(image_url):\n",
    "     return '<img src=\"{}\" style=\"display:inline;margin:1px\" width=\"100\" height=\"100\"/>'\\\n",
    "            .format(image_url)\n",
    "        \n",
    "def show_movies_for_user(userId, verbose=False, show_all=False):\n",
    "    \"\"\"Retrieve posters of top rated movies for userId.\n",
    "    \n",
    "    Note: this accepts the original user id\n",
    "    \"\"\"\n",
    "    html = ''\n",
    "    max_movies = 10\n",
    "    i=0\n",
    "    \n",
    "    user_movies = ratings_raw[ratings_raw.userId == userId][\"movieId\"]\n",
    "    print(\"User \", userId , \" watched \", len(user_movies), ' movies') \n",
    "    \n",
    "    user_movies = ratings_raw[ratings_raw.userId == userId].sort_values(\"rating\", ascending=False) \n",
    "    for index, row in user_movies.iterrows():\n",
    "        movieId = row[\"movieId\"]\n",
    "        movie_item = links[links.movieId == movieId]\n",
    "        tmdbId = movie_item[\"tmdbId\"].item()\n",
    "        if verbose:\n",
    "            print(movieId, tmdbId)\n",
    "        if np.isnan(tmdbId):\n",
    "            url = None\n",
    "        else:\n",
    "            url = TMDB().get_poster_path_by_id(int(tmdbId))\n",
    "        html = html + make_html(url)\n",
    "        i +=1\n",
    "        \n",
    "        if ~show_all and (i == max_movies):\n",
    "                break\n",
    "        \n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see which movies user 5 watches\n",
    "show_movies_for_user(5, verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helper functions\n",
    "def get_name_for_movie_id(movie_id):\n",
    "    \"\"\"Returns the name of a movie_id (based on ratings_raw)\"\"\"\n",
    "    \n",
    "    try: \n",
    "        movie_name = movies[movies.movieId == movie_id][\"title\"].item()\n",
    "    except KeyError:\n",
    "        movie_name = None\n",
    "    return movie_name\n",
    "    \n",
    "    \n",
    "def show_genres_histogram_for_user(user_id):\n",
    "    \"\"\"Create histogram of movies genres user_id has watched.\n",
    "    \n",
    "    Note: this uses the original user_id NOT the userIDX\n",
    "    \"\"\"\n",
    "    \n",
    "    user_movies = ratings_raw[ratings_raw.userId == user_id][\"movieId\"]\n",
    "    print(\"User \", user_id , \" watched \", len(user_movies), ' movies') \n",
    "    user_movies_with_genre = movies[movies.movieId.isin(user_movies)]\n",
    "    display(user_movies_with_genre)\n",
    "    \n",
    "    genres_list = []\n",
    "    for index, row in user_movies_with_genre.iterrows():\n",
    "        genres_list += row[\"genres\"].split('|')\n",
    "    \n",
    "    df_genres = pd.DataFrame(genres_list, columns=['genres'])\n",
    "    \n",
    "    df_genres.groupby('genres').size().sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can see what user 5 watches\n",
    "show_genres_histogram_for_user(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncate the data\n",
    "\n",
    "To speed things up, we will work with a *truncated* version of the data, containing up to 10000 users and 1000 movies. \n",
    "\n",
    "**Important:** To see results with the full dataset, set `DEBUG = False`, and **rerun all cells starting from the top**. But be careful this may take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "if DEBUG: \n",
    "    ratings_raw = ratings_raw[ (ratings_raw['userId'] < 10000) & (ratings_raw['movieId'] < 1000) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data\n",
    "Make sure that movies and users have consecutive indexes starting from 0. Also drop the timestamp column.\n",
    "\n",
    "The resulting \"cleaned\" data are stored in `ratings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieIds = ratings_raw.movieId.unique()\n",
    "movieIds.sort()\n",
    "userIds = ratings_raw.userId.unique()\n",
    "userIds.sort()\n",
    "\n",
    "m = userIds.size\n",
    "n = movieIds.size\n",
    "numRatings = len(ratings_raw)\n",
    "\n",
    "print (\"There are\", m, \"users,\", n, \"items and\", numRatings, \"ratings.\")\n",
    "\n",
    "\n",
    "## movies and users should have consecutive indexes starting from 0\n",
    "# dictionaries to convert move id to consecutive index and vice versa\n",
    "movieId_to_movieIDX = dict(zip(movieIds, range(0, movieIds.size)))\n",
    "movieIDX_to_movieId = dict(zip(range(0, movieIds.size), movieIds))\n",
    "\n",
    "# dictionaries to convert user id to consecutive index and vice versa\n",
    "userId_to_userIDX = dict(zip(userIds, range(0, userIds.size )))\n",
    "userIDX_to_userId = dict(zip(range(0, userIds.size), userIds))\n",
    "\n",
    "## drop timestamps\n",
    "ratings = pd.concat([ratings_raw['userId'].map(userId_to_userIDX),\n",
    "                     ratings_raw['movieId'].map(movieId_to_movieIDX),\n",
    "                     ratings_raw['rating']], axis=1)\n",
    "ratings.columns = ['user', 'item', 'rating']\n",
    "\n",
    "display(ratings.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "movieIDX_to_movieId[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Ratings Matrix\n",
    "\n",
    "We will convert the `ratings` `DataFrame` into a **Ratings Matrix**. Because it is very sparse, we will use the `scipy.sparse` module to efficiently store and access it.\n",
    "\n",
    "Specifically, we will create two versions of the same ratings matrix:\n",
    "- `R` is our basic matrix and is optimized for dot products, which will be useful when computing user-user similarities; `R` is stored in the Compressed Sparse Row format (`csr_matrix`).\n",
    "- `R_dok` is a different view of the ratings matrix, which allows to quickly test whether a user-item rating exists; `R_dok` is stored in the Dictionary Of Keys format (`dok_matrix`) so you can access the data like a dictionary (which is fast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = sp.csr_matrix((ratings.rating, (ratings.user, ratings.item))) # input is (data value, (index 0, index 1))\n",
    "R_dok = R.todok()\n",
    "# a simple test: user 0 item 534 should have a rating of 4\n",
    "print(\"R[0, 534] value is \", R[0, 534])\n",
    "print(\"R_dok[(0, 534)] value is \", R_dok[(0,534)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = R.shape[0]\n",
    "n = R.shape[1]\n",
    "numRatings = R.count_nonzero()\n",
    "\n",
    "print(\"There are m =\", m, \"users, n =\", n, \"items and\", numRatings, \"ratings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fun starts here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Average Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code computes the average rating of each user. This will be used for mean-centering, i.e., when computing similarities, as well as for making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sums = R.sum(axis=1).A1 ## matrix converted to 1-D array via .A1\n",
    "user_cnts = np.diff(R.indptr) ## equivalent to, but faster than: user_cnts = (R != 0).sum(axis=1).A1\n",
    "user_avgs = user_sums / user_cnts\n",
    "print(\"user_avgs\", user_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This thread](https://stackoverflow.com/questions/52299420/scipy-csr-matrix-understand-indptr) in stack overflow, explains the method `indptr` for a sparse matrix in scipy clearly. Recommended to read for those who are inerested or got confused with the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In what follows we will have 4 exercises for you. Just follow these exercises and try to fill in the missing parts of the code. after each part we have some examples with answers, so that you can sanity-check the functions that you have completed. This parts are marked by **DEBUG**. \n",
    "\n",
    "In the end you will have your own recommendar system, built from scratch. Have fun :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise1: compute User-User Similarity \n",
    "\n",
    "The following function computes the mean-centered cosine similarity between two users.\n",
    "\n",
    "*Tricks* that might be useful:\n",
    "\n",
    "To subtract a scalar value `y` from all nonzero entries of a sparse vector `x`, do:\n",
    "```\n",
    "x.data = x.data - y\n",
    "```\n",
    "\n",
    "The dot product of a sparse vector `x` to sparse vector `y` is:\n",
    "```\n",
    "x.dot(y.T)\n",
    "```\n",
    "\n",
    "The norm of a sparse vector `x` is:\n",
    "```\n",
    "norm(x)\n",
    "```\n",
    "\n",
    "\n",
    "If a sparse vector `x` has only a single item, you can access it by:\n",
    "```\n",
    "x.A.item()\n",
    "```\n",
    "\n",
    "Note that `x.A` returns the dense representation of sparse vector `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some tests\n",
    "u = R[1,:].copy() \n",
    "v = R[2,:].copy()\n",
    "print(type(u.data))\n",
    "print(u.dot(v.T))\n",
    "print(u.dot(v.T).A)\n",
    "print(u.dot(v.T).A.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_user_similarity(u_id, v_id):\n",
    "    \"\"\"Computes the cosine similarity between two user ids.\n",
    "    dot(A,B)/norm(A)*norm(B)\n",
    "    \"\"\"\n",
    "    u = R[u_id,:].copy()\n",
    "    v = R[v_id,:].copy()\n",
    "    \n",
    "    ########## START HERE ##########\n",
    "    ### compute the numerator and denominator\n",
    "    # first compute the numerator i.e. dot product of the mean centered arrays\n",
    "    u.data = u.data - user_avgs[u_id]\n",
    "    # TO DO\n",
    "    v.data = \n",
    "    \n",
    "    numerator =  ## dot product for sparse representations\n",
    "    \n",
    "    denominator = # norm(u) * norm(v)\n",
    "    \n",
    "    ##########  END HERE  ##########\n",
    "    \n",
    "    if denominator == 0:\n",
    "        similarity = 0.;\n",
    "    else:\n",
    "        similarity = numerator/denominator\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEBUG:** For the truncated dataset, the following should output ~ `0.03585`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    sim = compute_pairwise_user_similarity(2, 6)\n",
    "    print(sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: User to all Users Similarities \n",
    "\n",
    "The following function computes the mean-centered cosine similarities of a given user to all other users.\n",
    "\n",
    "You should use the `compute_pairwise_user_similarity` function defined above.\n",
    "\n",
    "\n",
    "You will get a **bonus point**, if you can avoid the for loop and **NOT** invoke `compute_pairwise_user_similarity`. The idea is to obtain a copy, say `R_copy`, of matrix `R` that has its rows mean-centered and normalized. This way the given user can be represented by a mean-centered and normalized vector `u`. Then, to obtain the similarity of the user to all others, one needs to take the dot product `R_copy.dot(u.T)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_user_similarities(u_id):\n",
    "    uU = np.empty((m,)) # we have m users\n",
    "\n",
    "    ########## START HERE ##########\n",
    "    ### you may use a for loop for this function\n",
    "    # TO DO\n",
    "    for v_id in range(m):\n",
    "        sim = # pairwise similarity u_id and v_id\n",
    "        uU[v_id] = sim\n",
    "    ##########  END HERE  ##########\n",
    "    \n",
    "    return uU\n",
    "\n",
    "def compute_user_similarities_fast(u_id):\n",
    "    uU = np.empty((m,))\n",
    "    \n",
    "    global user_avgs, user_cnts # we already derived these two vectors\n",
    "    \n",
    "    ########## START BONUS ##########\n",
    "    R_copy = R.copy() ## create a copy to work with\n",
    "    # TO DO: repeat each user_avg, user_cnt times\n",
    "    user_avgs_repeated = np.repeat()\n",
    "\n",
    "    R_copy.data -= user_avgs_repeated ## R_copy is now mean centered\n",
    "    \n",
    "    ## normalize rows to unit norm\n",
    "    # TO DO\n",
    "    R_copy = pp.normalize() ## normalize each row: elements divided by the row norm\n",
    "    u = R_copy[u_id, :]\n",
    "\n",
    "    uU = R_copy.dot(u.T).A.flatten() ## dot product; convert to dense matrix, then flatten\n",
    "    ##########  END BONUS  ##########\n",
    "    return uU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEBUG:** For the truncated dataset, the following should again output ~ `0.03585`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "if DEBUG:\n",
    "    start = time.time()\n",
    "    uU = compute_user_similarities(2)\n",
    "    print(\"Time=\", (time.time()-start))\n",
    "    \n",
    "    start = time.time()\n",
    "    uU = compute_user_similarities_fast(2)\n",
    "    print(\"Time=\", (time.time()-start))\n",
    "    \n",
    "    print(uU[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uU.shape); print(uU) ## uU stores the similarity of that user to all the other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(uU)[-2] # most similar user to user with user index 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the top 10 rated movies by user index 2 and user index 1871 which is the most similar user to user index 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_movies_for_user(userIDX_to_userId[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_movies_for_user(userIDX_to_userId[1871])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Create User Neighborhood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function creates the user neighborhood of a given user. It takes as input, the target user `u_id` and the target item`i_id`, and uses additional parameters, the size `k` of the neighborhood, and a flag `with_abs_sim`.\n",
    "\n",
    "If `with_abs_sim` is `True`, the neighborhood should contain up to `k` users with the highest absolute similarity to the target user `u_id`.\n",
    "\n",
    "If `with_abs_sim` is `False`, the neighborhood should contain up to `k` users with the highest similarity to the target user `u_id`.\n",
    "\n",
    "The output of the function is `nh`, a `Dictionary` containing key-value entries of the form `v_id : sim(u_id, v_id)`, where `v_id` is another user and `sim(u_id, v_id)` is the similarity between `u_id` and `v_id`.\n",
    "\n",
    "**Note:** The neighborhood of the target user should not contain itself, i.e., `u_id`, and only include users that have rated the target item `i_id`.\n",
    "\n",
    "\n",
    "*Tricks* that might be useful:\n",
    "\n",
    "`np.absolute(x)` returns an array containing the absolute values of each element in array `x`.\n",
    "\n",
    "`np.argsort(x)` returns an array with the indices that sort array `x` in *increasing* order.\n",
    "\n",
    "`x[::-1]` returns the reversed array of `x`. So, `np.argsort(x)[::-1]` contains the indices that sort x in *decreasing* order.\n",
    "\n",
    "To check if user `u_id` has rated item `i_id`, the `R_dok` view of the ratings matrix is helpful:\n",
    "```\n",
    "(u_id, i_id) in R_dok\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_user_neighborhood(u_id, i_id, k=5, with_abs_sim = False):\n",
    "    \"\"\" Neighborhood for user \"u_id\" that have also watched item \"i_id\"\n",
    "    \"\"\"\n",
    "    nh = {} ## the neighborhood dict with (user id: similarity) entries\n",
    "    ## nh should not contain u_id and only include users that have rated i_id; there should be at most k neighbors\n",
    "    \n",
    "    uU = compute_user_similarities(u_id)\n",
    "    uU_copy = uU.copy() ## so that we can modify it, but also keep the original\n",
    "    \n",
    "    ########## START HERE ##########\n",
    "    \n",
    "    if with_abs_sim:\n",
    "        uU_copy = np.absolute(uU_copy)  # we only care about the absolute value of the similarity\n",
    "    \n",
    "    # TO DO: derive the user_ids sorted by decreasing similarity (or absolute similarity) to u_id\n",
    "    user_ids =  \n",
    "    \n",
    "    count = 0\n",
    "    for v_id in user_ids:\n",
    "        if v_id == u_id: ## ignore self\n",
    "            continue # go to the next iterate of the loop \n",
    "        # TO DO: replace the 'condition' such that it ignores users that have not rated i_id\n",
    "        if condition: \n",
    "            continue\n",
    "        nh[v_id] = uU[v_id]\n",
    "        count += 1\n",
    "        if count == k:\n",
    "            break\n",
    "    \n",
    "    ##########  END HERE  ##########\n",
    "    \n",
    "    return nh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEBUG:** For the truncated dataset, the following should output sth like:\n",
    "```\n",
    "{550: 0.2971362420132689, 7231: 0.2740097242608031, 128: 0.26790790730529773, 6929: 0.25874452246587126, 3590: 0.251786413056871}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    print(get_name_for_movie_id(movie_id=movieIDX_to_movieId[0]))\n",
    "    nh = create_user_neighborhood(u_id=userId_to_userIDX[6], i_id=0)\n",
    "    print(nh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have seen what are the most similar users to user 6 that have also watched `Toy Story`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_movies_for_user(6)\n",
    "show_genres_histogram_for_user(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now we can see what movies the userIDX 2277 (the most similar to user_id=6) has watched.\n",
    "show_movies_for_user(userIDX_to_userId[2277])\n",
    "show_genres_histogram_for_user(userIDX_to_userId[2277])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Predict a Rating \n",
    "\n",
    "Finally! We can now try to predict a rating of a given user who has not rated an item so far. Follow the instructions bellow to see how we can do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function predicts the rating user `u_id` would give to item `i_id`. It uses the flag `with_deviations` to make the prediction.\n",
    "\n",
    "If `with_deviations` is `True`, the prediction is made over *rating deviations*:\n",
    "$$ s(u,i) = \\overline{r_u} + \\frac{\\sum_{v \\in N(u;i)}w_{uv} (r_{vi}-\\overline{r_v})}{\\sum_{v \\in N(u;i)} |w_{uv}|} .$$\n",
    "\n",
    "If `with_deviations` is `False`, the prediction is made directly over ratings:\n",
    "$$ s(u,i) = \\frac{\\sum_{v \\in N(u;i)}w_{uv} r_{vi}}{\\sum_{v \\in N(u;i)} |w_{uv}|} .$$\n",
    "\n",
    "The output of the function is the predicted rating `prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_rating(u_id, i_id, k, with_deviations=True, with_abs_sim=False):\n",
    "    '''\n",
    "    predict the rating of user u_id for item i_id \n",
    "    '''\n",
    "    \n",
    "    print(\"Movie name:\", get_name_for_movie_id(movie_id=movieIDX_to_movieId[i_id]))\n",
    "    \n",
    "    if (u_id, i_id) in R_dok:\n",
    "        print(\"user idx\", u_id, \"has rated item idx\", i_id, \"with\", R[u_id, i_id])\n",
    "    else:\n",
    "        print(\"user idx\", u_id, \"has not rated item idx\", i_id)\n",
    "    print(\"k:\", k, \"with_deviations:\", with_deviations, \"with_abs_sim:\", with_abs_sim)\n",
    "    \n",
    "    \n",
    "    nh = create_user_neighborhood(u_id, i_id, k=k, with_abs_sim=with_abs_sim)\n",
    "    \n",
    "    neighborhood_weighted_avg = 0.\n",
    "\n",
    "    ########## START HERE ##########\n",
    "    ### you should compute neighborhood_weighted_avg, which is the fraction in the above formulas\n",
    "    \n",
    "    sum_scores = 0.\n",
    "    sum_weights = 0.\n",
    "    for neighbor_id, similarity in nh.items():\n",
    "        # TO DO: find the neighbor rating from R matrix. R[neighbor_id, i_id]\n",
    "        neighbor_rating = \n",
    "        if with_deviations:\n",
    "            # TO DO: in this case similarity should be multiplied by (neighbor_rating - neighbor_avg)\n",
    "            sum_scores += \n",
    "        else:\n",
    "            sum_scores += similarity * neighbor_rating\n",
    "        sum_weights += abs(similarity)\n",
    "        \n",
    "    neighborhood_weighted_avg = sum_scores/sum_weights\n",
    "    \n",
    "    ##########  END HERE  ##########\n",
    "    \n",
    "    if with_deviations:\n",
    "        prediction = user_avgs[u_id] + neighborhood_weighted_avg\n",
    "        print(\"prediction\", prediction, \"(user_avg\", user_avgs[u_id], \"offset\", neighborhood_weighted_avg, \")\")\n",
    "    else:\n",
    "        prediction = neighborhood_weighted_avg\n",
    "        print(\"prediction\", prediction, \"(user_avg\", user_avgs[u_id], \")\")\n",
    "        \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEBUG:** For the truncated dataset, the following should output sth like:\n",
    "```\n",
    "Movie name: Sudden Death (1995)\n",
    "user 6 has not rated item 8\n",
    "k: 50 with_deviations: True with_abs_sim: True\n",
    "prediction 2.7554307504988773 (user_avg 3.2830188679245285 offset -0.527588117425651 )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_deviations = True\n",
    "if DEBUG:\n",
    "    k = 50\n",
    "    with_deviations = True\n",
    "    with_abs_sim = True\n",
    "    predict_rating(6, 8, k=50, with_deviations = with_deviations, with_abs_sim=with_abs_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    k = 50\n",
    "    with_deviations = True\n",
    "    with_abs_sim = True\n",
    "    predict_rating(6, movieId_to_movieIDX[5], k=50, with_deviations = with_deviations, with_abs_sim=with_abs_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
